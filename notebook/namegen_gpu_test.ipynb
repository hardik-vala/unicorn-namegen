{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnbOyb7V/IOgqL/tCWFGAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4572f956d9634f8f8d1dc3bb98b01ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3e5600c8a1a432ca78e728ce5e93920",
              "IPY_MODEL_b472e702d17247d08583efe088740216"
            ],
            "layout": "IPY_MODEL_d64157b571f448b0aaa462667e99fa04"
          }
        },
        "f3e5600c8a1a432ca78e728ce5e93920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59e0fe5cb6f14201a68d07621c12190e",
            "placeholder": "​",
            "style": "IPY_MODEL_54b7a778555c49c69f4fa843b800ef91",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "b472e702d17247d08583efe088740216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a36a0b8ab9a64e05939cc56a756e9f31",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74824ffaebc2468289313366d6ca526c",
            "value": 1
          }
        },
        "d64157b571f448b0aaa462667e99fa04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59e0fe5cb6f14201a68d07621c12190e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54b7a778555c49c69f4fa843b800ef91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a36a0b8ab9a64e05939cc56a756e9f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74824ffaebc2468289313366d6ca526c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardik-vala/unicorn-namegen/blob/main/notebook/namegen_gpu_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbmAxLX_8c25",
        "outputId": "5656ed7b-a31e-4370-e6e8-9b29b15564cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.1.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgev0-bCvdmr"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        head_size = config.n_embd // config.n_head\n",
        "        self.key = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\n",
        "            \"tril\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_head)])\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, C)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculate the forward pass\n",
        "        xmean = x.mean(1, keepdim=True)\n",
        "        xvar = x.var(1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block: Communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(config)\n",
        "        self.ffwd = FeedForward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2.\n",
        "\n",
        "\n",
        "class Namegen(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "        modules = [Block(config) for _ in range(config.n_layer)] + [\n",
        "            nn.LayerNorm(config.n_embd)\n",
        "        ]\n",
        "        self.blocks = nn.Sequential(*modules)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "        self.ffwd = FeedForward(config)\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos = torch.arange(T, device=device)\n",
        "        pos_emb = self.position_embedding_table(pos)  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        # fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        # use_fused = fused_available and device_type == 'cuda'\n",
        "        # extra_args = dict(fused=True) if use_fused else dict()\n",
        "        extra_args = dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        # print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size :]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# I/O\n",
        "out_dir = \"out\"\n",
        "eval_iters = 200\n",
        "log_interval = 1000\n",
        "# wandb logging\n",
        "wandb_log = True\n",
        "wandb_project = \"namegen\"\n",
        "wandb_run_name = \"507-90V\"\n",
        "# data\n",
        "dataset = \"names\"\n",
        "batch_size = 90\n",
        "block_size = 20  # context length\n",
        "vocab_size = 90\n",
        "# model\n",
        "n_layer = 7\n",
        "n_head = 6\n",
        "n_embd = 84\n",
        "dropout = 0.2  # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "write_checkpoint = True\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-4  # max learning rate\n",
        "max_iters = 42000\n",
        "weight_decay=1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "# system\n",
        "device = (\n",
        "    \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        ")\n",
        "# ------------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}  # for logging\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "tokens_per_iter = batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "torch.manual_seed(24)\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
        "ctx = nullcontext()\n",
        "\n",
        "# data loader\n",
        "data_dir = os.path.join(\"data\", dataset)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = np.memmap(os.path.join(\"data\", \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(\"data\", \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + block_size + 1]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(\"data\", \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=meta_vocab_size,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "modelconf = ModelConfig(**model_args)\n",
        "model = Namegen(modelconf)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# logging\n",
        "if wandb_log:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "\n",
        "# training loop\n",
        "t0 = time.time()\n",
        "for step in range(max_iters):\n",
        "    if step % log_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(\n",
        "            f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\"step\": step, \"train/loss\": losses[\"train\"], \"val/loss\": losses[\"val\"]}\n",
        "            )\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# timing\n",
        "t1 = time.time()\n",
        "dt = t1 - t0\n",
        "print(f\"time: {dt:.2f}s\")\n",
        "\n",
        "# write checkpoint\n",
        "if write_checkpoint:\n",
        "    checkpoint = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"model_args\": model_args,\n",
        "        \"iter_num\": step,\n",
        "        \"final_val_loss\": losses[\"val\"],\n",
        "        \"config\": config,\n",
        "    }\n",
        "    print(f\"saving checkpoint to {out_dir}\")\n",
        "    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4572f956d9634f8f8d1dc3bb98b01ab0",
            "f3e5600c8a1a432ca78e728ce5e93920",
            "b472e702d17247d08583efe088740216",
            "d64157b571f448b0aaa462667e99fa04",
            "59e0fe5cb6f14201a68d07621c12190e",
            "54b7a778555c49c69f4fa843b800ef91",
            "a36a0b8ab9a64e05939cc56a756e9f31",
            "74824ffaebc2468289313366d6ca526c"
          ]
        },
        "id": "uMBxemyayvOW",
        "outputId": "5678fcc1-bd11-45e5-cabf-0156f448ea16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 1,800\n",
            "found vocab_size = 90 (inside data/meta.pkl)\n",
            "num decayed parameter tensors: 152, with 665,952 parameters\n",
            "num non-decayed parameter tensors: 54, with 6,558 parameters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:mogp0i5t) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4572f956d9634f8f8d1dc3bb98b01ab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>39000</td></tr><tr><td>train/loss</td><td>2.15675</td></tr><tr><td>val/loss</td><td>2.18135</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">507-85V-unicorn</strong> at: <a href='https://wandb.ai/hardikvala/namegen/runs/mogp0i5t' target=\"_blank\">https://wandb.ai/hardikvala/namegen/runs/mogp0i5t</a><br/> View project at: <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">https://wandb.ai/hardikvala/namegen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240508_015423-mogp0i5t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:mogp0i5t). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240508_024310-wt1003tq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hardikvala/namegen/runs/wt1003tq' target=\"_blank\">507-90V</a></strong> to <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">https://wandb.ai/hardikvala/namegen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hardikvala/namegen/runs/wt1003tq' target=\"_blank\">https://wandb.ai/hardikvala/namegen/runs/wt1003tq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6849, val loss 4.6839\n",
            "step 1000: train loss 3.1028, val loss 3.1029\n",
            "step 2000: train loss 2.9825, val loss 2.9850\n",
            "step 3000: train loss 2.8709, val loss 2.8744\n",
            "step 4000: train loss 2.7667, val loss 2.7654\n",
            "step 5000: train loss 2.6796, val loss 2.6942\n",
            "step 6000: train loss 2.6195, val loss 2.6261\n",
            "step 7000: train loss 2.5658, val loss 2.5794\n",
            "step 8000: train loss 2.5259, val loss 2.5474\n",
            "step 9000: train loss 2.5014, val loss 2.5138\n",
            "step 10000: train loss 2.4655, val loss 2.4936\n",
            "step 11000: train loss 2.4437, val loss 2.4557\n",
            "step 12000: train loss 2.4222, val loss 2.4355\n",
            "step 13000: train loss 2.4045, val loss 2.4224\n",
            "step 14000: train loss 2.3946, val loss 2.4050\n",
            "step 15000: train loss 2.3797, val loss 2.3897\n",
            "step 16000: train loss 2.3591, val loss 2.3804\n",
            "step 17000: train loss 2.3513, val loss 2.3678\n",
            "step 18000: train loss 2.3492, val loss 2.3708\n",
            "step 19000: train loss 2.3294, val loss 2.3546\n",
            "step 20000: train loss 2.3193, val loss 2.3541\n",
            "step 21000: train loss 2.3064, val loss 2.3381\n",
            "step 22000: train loss 2.3009, val loss 2.3314\n",
            "step 23000: train loss 2.2913, val loss 2.3196\n",
            "step 24000: train loss 2.2912, val loss 2.3170\n",
            "step 25000: train loss 2.2786, val loss 2.3039\n",
            "step 26000: train loss 2.2747, val loss 2.2987\n",
            "step 27000: train loss 2.2603, val loss 2.2969\n",
            "step 28000: train loss 2.2691, val loss 2.2959\n",
            "step 29000: train loss 2.2517, val loss 2.2950\n",
            "step 30000: train loss 2.2534, val loss 2.2770\n",
            "step 31000: train loss 2.2523, val loss 2.2722\n",
            "step 32000: train loss 2.2486, val loss 2.2711\n",
            "step 33000: train loss 2.2316, val loss 2.2664\n",
            "step 34000: train loss 2.2247, val loss 2.2699\n",
            "step 35000: train loss 2.2373, val loss 2.2727\n",
            "step 36000: train loss 2.2237, val loss 2.2515\n",
            "step 37000: train loss 2.2212, val loss 2.2679\n",
            "step 38000: train loss 2.2139, val loss 2.2573\n",
            "step 39000: train loss 2.2077, val loss 2.2565\n",
            "step 40000: train loss 2.2186, val loss 2.2464\n",
            "step 41000: train loss 2.2113, val loss 2.2454\n",
            "time: 2809.56s\n",
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = \"out\"  # ignored if init_from is not 'resume'\n",
        "num_samples = 10  # number of samples to draw\n",
        "max_new_tokens = 100  # number of tokens generated in each sample\n",
        "temperature = (\n",
        "    0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        ")\n",
        "top_k = (\n",
        "    200  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        ")\n",
        "seed = 24\n",
        "device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# torch.manual_seed(seed)\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
        "ctx = nullcontext()\n",
        "\n",
        "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "modelconf = ModelConfig(**checkpoint[\"model_args\"])\n",
        "model = Namegen(modelconf)\n",
        "state_dict = checkpoint[\"model\"]\n",
        "unwanted_prefix = \"_orig_mod.\"\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "meta_path = os.path.join(\"data\", \"meta.pkl\")\n",
        "print(f\"Loading meta from {meta_path}...\")\n",
        "with open(meta_path, \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "merges = meta[\"merges\"]\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "encode1 = lambda s: [stoi[c] for c in s]\n",
        "decode1 = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "def unmerge(ids, pair, idx):\n",
        "    newids = []\n",
        "    for i in ids:\n",
        "        if i == idx:\n",
        "            newids.append(pair[0])\n",
        "            newids.append(pair[1])\n",
        "        else:\n",
        "            newids.append(i)\n",
        "    return newids\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "    tokens = list(ids)\n",
        "    for pair, idx in reversed(merges.items()):\n",
        "        tokens = unmerge(tokens, pair, idx)\n",
        "    return decode1(tokens)\n",
        "\n",
        "sample_cnt = 0\n",
        "with torch.no_grad():\n",
        "    while True:\n",
        "        x = torch.full((1, 1), stoi[\"!\"], dtype=torch.long, device=device)\n",
        "        y = model.generate(x, max_new_tokens)\n",
        "        raw = decode(y[0].tolist())\n",
        "        parts = raw.split(\"!\")\n",
        "        for i in range(1, len(parts) - 1):\n",
        "            print(parts[i])\n",
        "            sample_cnt += 1\n",
        "            if sample_cnt >= num_samples:\n",
        "                break\n",
        "        if sample_cnt >= num_samples:\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzfaJWK_-pu",
        "outputId": "7fbe81fc-bef7-44ee-eb5e-2e8428a9baf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta from data/meta.pkl...\n",
            "Jobster Technical\n",
            "Colliaotle SpendBout\n",
            "Descount\n",
            "Dogqi\n",
            "Sedruob\n",
            "Solpt Pava\n",
            "Privatoriewtor\n",
            "Equs\n",
            "Counsect\n",
            "First BlueTechnologies\n"
          ]
        }
      ]
    }
  ]
}