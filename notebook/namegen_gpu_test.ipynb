{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOap7Sod1ztBISQJM+MT3zL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardik-vala/unicorn-namegen/blob/main/notebook/namegen_gpu_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbmAxLX_8c25",
        "outputId": "42646dbe-6759-4224-8f7e-7fec756a4660"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.45.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tgev0-bCvdmr"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        head_size = config.n_embd // config.n_head\n",
        "        self.key = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\n",
        "            \"tril\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_head)])\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, C)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculate the forward pass\n",
        "        xmean = x.mean(1, keepdim=True)\n",
        "        xvar = x.var(1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block: Communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(config)\n",
        "        self.ffwd = FeedForward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2.\n",
        "\n",
        "\n",
        "class Namegen(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "        modules = [Block(config) for _ in range(config.n_layer)] + [\n",
        "            nn.LayerNorm(config.n_embd)\n",
        "        ]\n",
        "        self.blocks = nn.Sequential(*modules)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "        self.ffwd = FeedForward(config)\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos = torch.arange(T, device=device)\n",
        "        pos_emb = self.position_embedding_table(pos)  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size :]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# I/O\n",
        "out_dir = \"out\"\n",
        "eval_iters = 200\n",
        "log_interval = 500\n",
        "# wandb logging\n",
        "wandb_log = True\n",
        "wandb_project = \"namegen\"\n",
        "wandb_run_name = \"418-gpu\"\n",
        "# data\n",
        "dataset = \"names\"\n",
        "batch_size = 40\n",
        "block_size = 20  # context length\n",
        "# model\n",
        "n_layer = 5\n",
        "n_head = 5\n",
        "n_embd = 50\n",
        "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "write_checkpoint = True\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-4  # max learning rate\n",
        "max_iters = 10000\n",
        "# system\n",
        "device = (\n",
        "    # GPU\n",
        "    \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        ")\n",
        "# ------------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}  # for logging\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "tokens_per_iter = batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "torch.manual_seed(24)\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
        "ctx = nullcontext()\n",
        "\n",
        "# data loader\n",
        "data_dir = os.path.join(\"data\", dataset)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = np.memmap(os.path.join(\"data\", \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(\"data\", \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + block_size + 1]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(\"data\", \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=meta_vocab_size,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "modelconf = ModelConfig(**model_args)\n",
        "model = Namegen(modelconf)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# logging\n",
        "if wandb_log:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "\n",
        "# training loop\n",
        "t0 = time.time()\n",
        "for step in range(max_iters):\n",
        "    if step % log_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(\n",
        "            f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\"step\": step, \"train/loss\": losses[\"train\"], \"val/loss\": losses[\"val\"]}\n",
        "            )\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# timing\n",
        "t1 = time.time()\n",
        "dt = t1 - t0\n",
        "print(f\"time: {dt:.2f}s\")\n",
        "\n",
        "# write checkpoint\n",
        "if write_checkpoint:\n",
        "    checkpoint = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"model_args\": model_args,\n",
        "        \"iter_num\": step,\n",
        "        \"final_val_loss\": losses[\"val\"],\n",
        "        \"config\": config,\n",
        "    }\n",
        "    print(f\"saving checkpoint to {out_dir}\")\n",
        "    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "uMBxemyayvOW",
        "outputId": "59213764-24ad-44f7-ea0b-c0b4a9811d02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 800\n",
            "found vocab_size = 71 (inside data/meta.pkl)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240418_204737-nmly41vu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hardikvala/namegen/runs/nmly41vu' target=\"_blank\">418-gpu</a></strong> to <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">https://wandb.ai/hardikvala/namegen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hardikvala/namegen/runs/nmly41vu' target=\"_blank\">https://wandb.ai/hardikvala/namegen/runs/nmly41vu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4171, val loss 4.4434\n",
            "step 500: train loss 2.9360, val loss 2.8890\n",
            "step 1000: train loss 2.7159, val loss 2.6827\n",
            "step 1500: train loss 2.6184, val loss 2.5907\n",
            "step 2000: train loss 2.5290, val loss 2.5181\n",
            "step 2500: train loss 2.4640, val loss 2.4514\n",
            "step 3000: train loss 2.3916, val loss 2.3789\n",
            "step 3500: train loss 2.3348, val loss 2.3435\n",
            "step 4000: train loss 2.2765, val loss 2.3046\n",
            "step 4500: train loss 2.2316, val loss 2.2614\n",
            "step 5000: train loss 2.2104, val loss 2.2386\n",
            "step 5500: train loss 2.1821, val loss 2.2066\n",
            "step 6000: train loss 2.1455, val loss 2.1996\n",
            "step 6500: train loss 2.1241, val loss 2.1677\n",
            "step 7000: train loss 2.0776, val loss 2.1448\n",
            "step 7500: train loss 2.0522, val loss 2.1360\n",
            "step 8000: train loss 2.0335, val loss 2.1259\n",
            "step 8500: train loss 2.0289, val loss 2.1202\n",
            "step 9000: train loss 1.9870, val loss 2.0983\n",
            "step 9500: train loss 1.9702, val loss 2.0883\n",
            "time: 449.75s\n",
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = \"out\"  # ignored if init_from is not 'resume'\n",
        "num_samples = 10  # number of samples to draw\n",
        "max_new_tokens = 100  # number of tokens generated in each sample\n",
        "temperature = (\n",
        "    0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        ")\n",
        "top_k = (\n",
        "    200  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        ")\n",
        "seed = 24\n",
        "device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
        "ctx = nullcontext()\n",
        "\n",
        "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "modelconf = ModelConfig(**checkpoint[\"model_args\"])\n",
        "model = Namegen(modelconf)\n",
        "state_dict = checkpoint[\"model\"]\n",
        "unwanted_prefix = \"_orig_mod.\"\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "meta_path = os.path.join(\"data\", \"meta.pkl\")\n",
        "print(f\"Loading meta from {meta_path}...\")\n",
        "with open(meta_path, \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "sample_cnt = 0\n",
        "with torch.no_grad():\n",
        "    while True:\n",
        "        x = torch.full((1, 1), stoi[\"!\"], dtype=torch.long, device=device)\n",
        "        y = model.generate(x, max_new_tokens)\n",
        "        raw = decode(y[0].tolist())\n",
        "        parts = raw.split(\"!\")\n",
        "        for i in range(1, len(parts) - 1):\n",
        "            print(\"---------------\")\n",
        "            print(parts[i])\n",
        "            sample_cnt += 1\n",
        "            if sample_cnt >= num_samples:\n",
        "                break\n",
        "        if sample_cnt >= num_samples:\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzfaJWK_-pu",
        "outputId": "d1960fa2-5197-40e9-8be3-891430cb053e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta from data/meta.pkl...\n",
            "---------------\n",
            "Roges Dettonital Cock Pantcore\n",
            "---------------\n",
            "Vific Internaty\n",
            "---------------\n",
            "Gridge\n",
            "---------------\n",
            "Yurghei\n",
            "---------------\n",
            "Uniit.cmars\n",
            "---------------\n",
            "Banwix\n",
            "---------------\n",
            "\n",
            "---------------\n",
            "JAkC\n",
            "---------------\n",
            "HCL\n",
            "---------------\n",
            "Nist Trudstristu\n"
          ]
        }
      ]
    }
  ]
}