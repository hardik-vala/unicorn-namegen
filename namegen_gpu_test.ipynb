{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2HCq5AsYZEXZU2aqS07ON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardik-vala/unicorn-namegen/blob/main/namegen_gpu_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbmAxLX_8c25",
        "outputId": "0207f9ab-eda0-479e-a918-9483cea8d08e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-2.1.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgev0-bCvdmr"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        head_size = config.n_embd // config.n_head\n",
        "        self.key = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\n",
        "            \"tril\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_head)])\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, C)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LayerNorm:\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculate the forward pass\n",
        "        xmean = x.mean(1, keepdim=True)\n",
        "        xvar = x.var(1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize to unit variance\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block: Communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(config)\n",
        "        self.ffwd = FeedForward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2.\n",
        "\n",
        "\n",
        "class Namegen(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
        "        modules = [Block(config) for _ in range(config.n_layer)] + [\n",
        "            nn.LayerNorm(config.n_embd)\n",
        "        ]\n",
        "        self.blocks = nn.Sequential(*modules)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "        self.ffwd = FeedForward(config)\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos = torch.arange(T, device=device)\n",
        "        pos_emb = self.position_embedding_table(pos)  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        # fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        # use_fused = fused_available and device_type == 'cuda'\n",
        "        # extra_args = dict(fused=True) if use_fused else dict()\n",
        "        extra_args = dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        # print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size :]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# I/O\n",
        "out_dir = \"out\"\n",
        "eval_iters = 200\n",
        "log_interval = 1000\n",
        "# wandb logging\n",
        "wandb_log = True\n",
        "wandb_project = \"namegen\"\n",
        "wandb_run_name = \"506-80V-gpu\"\n",
        "# data\n",
        "dataset = \"names\"\n",
        "batch_size = 64\n",
        "block_size = 20  # context length\n",
        "vocab_size = 80\n",
        "# model\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 84\n",
        "dropout = 0.2  # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
        "write_checkpoint = True\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-4  # max learning rate\n",
        "max_iters = 40000\n",
        "weight_decay=1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "# system\n",
        "device = (\n",
        "    \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        ")\n",
        "# ------------------------------------------------------------------------------\n",
        "config_keys = [\n",
        "    k\n",
        "    for k, v in globals().items()\n",
        "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
        "]\n",
        "config = {k: globals()[k] for k in config_keys}  # for logging\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "tokens_per_iter = batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "torch.manual_seed(24)\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
        "ctx = nullcontext()\n",
        "\n",
        "# data loader\n",
        "data_dir = os.path.join(\"data\", dataset)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = np.memmap(os.path.join(\"data\", \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(\"data\", \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack(\n",
        "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "    )\n",
        "    y = torch.stack(\n",
        "        [\n",
        "            torch.from_numpy((data[i + 1 : i + block_size + 1]).astype(np.int64))\n",
        "            for i in ix\n",
        "        ]\n",
        "    )\n",
        "    if device_type == \"cuda\":\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(\"data\", \"meta.pkl\")\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta[\"vocab_size\"]\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=meta_vocab_size,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "modelconf = ModelConfig(**model_args)\n",
        "model = Namegen(modelconf)\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "              logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# logging\n",
        "if wandb_log:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "\n",
        "# training loop\n",
        "t0 = time.time()\n",
        "for step in range(max_iters):\n",
        "    if step % log_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(\n",
        "            f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "        if wandb_log:\n",
        "            wandb.log(\n",
        "                {\"step\": step, \"train/loss\": losses[\"train\"], \"val/loss\": losses[\"val\"]}\n",
        "            )\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# timing\n",
        "t1 = time.time()\n",
        "dt = t1 - t0\n",
        "print(f\"time: {dt:.2f}s\")\n",
        "\n",
        "# write checkpoint\n",
        "if write_checkpoint:\n",
        "    checkpoint = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"model_args\": model_args,\n",
        "        \"iter_num\": step,\n",
        "        \"final_val_loss\": losses[\"val\"],\n",
        "        \"config\": config,\n",
        "    }\n",
        "    print(f\"saving checkpoint to {out_dir}\")\n",
        "    torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uMBxemyayvOW",
        "outputId": "2abcfcaa-7d5f-4086-b514-4ff47e40b8ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 1,280\n",
            "found vocab_size = 80 (inside data/meta.pkl)\n",
            "num decayed parameter tensors: 131, with 579,600 parameters\n",
            "num non-decayed parameter tensors: 47, with 5,708 parameters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240507_033240-a0g7ma2h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hardikvala/namegen/runs/a0g7ma2h' target=\"_blank\">506-80V-gpu</a></strong> to <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hardikvala/namegen' target=\"_blank\">https://wandb.ai/hardikvala/namegen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hardikvala/namegen/runs/a0g7ma2h' target=\"_blank\">https://wandb.ai/hardikvala/namegen/runs/a0g7ma2h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4896, val loss 4.4908\n",
            "step 1000: train loss 2.9118, val loss 2.9180\n",
            "step 2000: train loss 2.8097, val loss 2.8125\n",
            "step 3000: train loss 2.7186, val loss 2.7196\n",
            "step 4000: train loss 2.6344, val loss 2.6404\n",
            "step 5000: train loss 2.5595, val loss 2.5786\n",
            "step 6000: train loss 2.5011, val loss 2.5197\n",
            "step 7000: train loss 2.4515, val loss 2.4698\n",
            "step 8000: train loss 2.4206, val loss 2.4421\n",
            "step 9000: train loss 2.3817, val loss 2.4063\n",
            "step 10000: train loss 2.3488, val loss 2.3805\n",
            "step 11000: train loss 2.3344, val loss 2.3620\n",
            "step 12000: train loss 2.3190, val loss 2.3421\n",
            "step 13000: train loss 2.2970, val loss 2.3299\n",
            "step 14000: train loss 2.2739, val loss 2.3088\n",
            "step 15000: train loss 2.2665, val loss 2.2964\n",
            "step 16000: train loss 2.2531, val loss 2.2826\n",
            "step 17000: train loss 2.2351, val loss 2.2611\n",
            "step 18000: train loss 2.2312, val loss 2.2600\n",
            "step 19000: train loss 2.2145, val loss 2.2479\n",
            "step 20000: train loss 2.1993, val loss 2.2480\n",
            "step 21000: train loss 2.1976, val loss 2.2412\n",
            "step 22000: train loss 2.1824, val loss 2.2338\n",
            "step 23000: train loss 2.1815, val loss 2.2205\n",
            "step 24000: train loss 2.1733, val loss 2.2127\n",
            "step 25000: train loss 2.1614, val loss 2.2203\n",
            "step 26000: train loss 2.1592, val loss 2.2066\n",
            "step 27000: train loss 2.1510, val loss 2.2034\n",
            "step 28000: train loss 2.1570, val loss 2.1958\n",
            "step 29000: train loss 2.1438, val loss 2.1882\n",
            "step 30000: train loss 2.1330, val loss 2.1804\n",
            "step 31000: train loss 2.1366, val loss 2.1666\n",
            "step 32000: train loss 2.1260, val loss 2.1747\n",
            "step 33000: train loss 2.1214, val loss 2.1713\n",
            "step 34000: train loss 2.1148, val loss 2.1677\n",
            "step 35000: train loss 2.1151, val loss 2.1577\n",
            "step 36000: train loss 2.1140, val loss 2.1559\n",
            "step 37000: train loss 2.1162, val loss 2.1602\n",
            "step 38000: train loss 2.0961, val loss 2.1533\n",
            "step 39000: train loss 2.0947, val loss 2.1558\n",
            "time: 2191.33s\n",
            "saving checkpoint to out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "out_dir = \"out\"  # ignored if init_from is not 'resume'\n",
        "num_samples = 10  # number of samples to draw\n",
        "max_new_tokens = 100  # number of tokens generated in each sample\n",
        "temperature = (\n",
        "    0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        ")\n",
        "top_k = (\n",
        "    200  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        ")\n",
        "seed = 24\n",
        "device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# torch.manual_seed(seed)\n",
        "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
        "ctx = nullcontext()\n",
        "\n",
        "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "modelconf = ModelConfig(**checkpoint[\"model_args\"])\n",
        "model = Namegen(modelconf)\n",
        "state_dict = checkpoint[\"model\"]\n",
        "unwanted_prefix = \"_orig_mod.\"\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "meta_path = os.path.join(\"data\", \"meta.pkl\")\n",
        "print(f\"Loading meta from {meta_path}...\")\n",
        "with open(meta_path, \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "merges = meta[\"merges\"]\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "encode1 = lambda s: [stoi[c] for c in s]\n",
        "decode1 = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "def unmerge(ids, pair, idx):\n",
        "    newids = []\n",
        "    for i in ids:\n",
        "        if i == idx:\n",
        "            newids.append(pair[0])\n",
        "            newids.append(pair[1])\n",
        "        else:\n",
        "            newids.append(i)\n",
        "    return newids\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "    tokens = list(ids)\n",
        "    for pair, idx in reversed(merges.items()):\n",
        "        tokens = unmerge(tokens, pair, idx)\n",
        "    return decode1(tokens)\n",
        "\n",
        "sample_cnt = 0\n",
        "with torch.no_grad():\n",
        "    while True:\n",
        "        x = torch.full((1, 1), stoi[\"!\"], dtype=torch.long, device=device)\n",
        "        y = model.generate(x, max_new_tokens)\n",
        "        raw = decode(y[0].tolist())\n",
        "        parts = raw.split(\"!\")\n",
        "        for i in range(1, len(parts) - 1):\n",
        "            print(parts[i])\n",
        "            sample_cnt += 1\n",
        "            if sample_cnt >= num_samples:\n",
        "                break\n",
        "        if sample_cnt >= num_samples:\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzfaJWK_-pu",
        "outputId": "ebf833dd-8dbd-4936-ac85-a783d9fefb10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta from data/meta.pkl...\n",
            "Surfish Diagnostics\n",
            "Upucom\n",
            "Pavix Jewell\n",
            "Gorge Hands OverWakers\n",
            "CarmaCental LLS\n",
            "VID\n",
            "dy Lee\n",
            "Basey Lea Holding\n",
            "PartnerApartments Partners\n",
            "TriLabs\n"
          ]
        }
      ]
    }
  ]
}